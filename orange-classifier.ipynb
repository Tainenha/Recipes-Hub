{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "157e70f8",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-15T16:31:55.683113Z",
     "iopub.status.busy": "2025-01-15T16:31:55.682802Z",
     "iopub.status.idle": "2025-01-15T16:35:26.516204Z",
     "shell.execute_reply": "2025-01-15T16:35:26.515440Z"
    },
    "papermill": {
     "duration": 210.837539,
     "end_time": "2025-01-15T16:35:26.517595",
     "exception": false,
     "start_time": "2025-01-15T16:31:55.680056",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1761 images belonging to 2 classes.\n",
      "Found 440 images belonging to 2 classes.\n",
      "Found 400 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 697ms/step - accuracy: 0.6805 - loss: 1.0962 - val_accuracy: 0.5769 - val_loss: 0.6971 - learning_rate: 1.0000e-04\n",
      "Epoch 2/30\n",
      "\u001b[1m 1/55\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.8438 - loss: 0.4288"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 84ms/step - accuracy: 0.8438 - loss: 0.4288 - val_accuracy: 0.6250 - val_loss: 0.6555 - learning_rate: 1.0000e-04\n",
      "Epoch 3/30\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 452ms/step - accuracy: 0.8534 - loss: 0.5825 - val_accuracy: 0.6058 - val_loss: 0.6707 - learning_rate: 1.0000e-04\n",
      "Epoch 4/30\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8125 - loss: 0.7213 - val_accuracy: 0.4583 - val_loss: 0.7319 - learning_rate: 1.0000e-04\n",
      "Epoch 5/30\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 431ms/step - accuracy: 0.8821 - loss: 0.4005 - val_accuracy: 0.5865 - val_loss: 0.8115 - learning_rate: 1.0000e-04\n",
      "Epoch 6/30\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 72ms/step - accuracy: 1.0000 - loss: 0.0554 - val_accuracy: 0.7500 - val_loss: 0.6354 - learning_rate: 2.0000e-05\n",
      "Epoch 7/30\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 434ms/step - accuracy: 0.9206 - loss: 0.2702 - val_accuracy: 0.6130 - val_loss: 1.0834 - learning_rate: 2.0000e-05\n",
      "Epoch 8/30\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9688 - loss: 0.3478 - val_accuracy: 0.6667 - val_loss: 0.9017 - learning_rate: 2.0000e-05\n",
      "Epoch 9/30\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 429ms/step - accuracy: 0.9277 - loss: 0.2322 - val_accuracy: 0.6731 - val_loss: 0.7776 - learning_rate: 2.0000e-05\n",
      "Epoch 10/30\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8750 - loss: 0.3254 - val_accuracy: 0.6250 - val_loss: 0.7809 - learning_rate: 1.0000e-05\n",
      "Epoch 11/30\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 431ms/step - accuracy: 0.9661 - loss: 0.1297 - val_accuracy: 0.6995 - val_loss: 0.7690 - learning_rate: 1.0000e-05\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 260ms/step - accuracy: 0.7329 - loss: 0.5193\n",
      "Test Accuracy: 0.6225\n"
     ]
    }
   ],
   "source": [
    "# Import các thư viện cần thiết\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "# Đường dẫn đến dữ liệu\n",
    "train_dir = '/kaggle/input/ai-training-challenge-hutech-orange-classifier/old_oranges_data_1/old_oranges_data/train_set'  \n",
    "test_dir = '/kaggle/input/ai-training-challenge-hutech-orange-classifier/old_oranges_data_1/old_oranges_data/test_set'    \n",
    "\n",
    "# Tiền xử lý dữ liệu\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    brightness_range=[0.8, 1.2],\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Tạo generator cho tập train và validation\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(224, 224),  # AlexNet yêu cầu kích thước ảnh 224x224\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "# Tạo generator cho tập test\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Tính toán class weights để xử lý mất cân bằng dữ liệu\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(train_generator.classes),\n",
    "    y=train_generator.classes\n",
    ")\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "# Xây dựng mô hình AlexNet\n",
    "model = Sequential([\n",
    "    # Layer 1\n",
    "    Conv2D(96, (11, 11), strides=(4, 4), activation='relu', input_shape=(224, 224, 3)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(3, 3), strides=(2, 2)),\n",
    "\n",
    "    # Layer 2\n",
    "    Conv2D(256, (5, 5), activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(3, 3), strides=(2, 2)),\n",
    "\n",
    "    # Layer 3\n",
    "    Conv2D(384, (3, 3), activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    # Layer 4\n",
    "    Conv2D(384, (3, 3), activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    # Layer 5\n",
    "    Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(3, 3), strides=(2, 2)),\n",
    "\n",
    "    # Fully Connected Layers\n",
    "    Flatten(),\n",
    "    Dense(4096, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(4096, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')  # Lớp đầu ra cho bài toán nhị phân\n",
    "])\n",
    "\n",
    "# Biên dịch mô hình\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint('alexnet_best_model.keras', monitor='val_accuracy', save_best_only=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.00001)\n",
    "\n",
    "# Huấn luyện mô hình\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // validation_generator.batch_size,\n",
    "    epochs=30,\n",
    "    callbacks=[early_stopping, checkpoint, reduce_lr],\n",
    "    class_weight=class_weights\n",
    ")\n",
    "\n",
    "# Đánh giá mô hình trên tập test\n",
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "print(f'Test Accuracy: {test_acc:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 10639231,
     "sourceId": 90828,
     "sourceType": "competition"
    },
    {
     "datasetId": 6397903,
     "sourceId": 10333407,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6400087,
     "sourceId": 10343317,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 214.705591,
   "end_time": "2025-01-15T16:35:28.256004",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-01-15T16:31:53.550413",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
